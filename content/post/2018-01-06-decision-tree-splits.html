---
title: "How Does a Decision Tree Choose a Split?"
author: "Joshua Loyal"
date: '2018-01-06'
slug: decision-tree-splits
draft: true
tags:
- Decision Trees
- Random Forest
categories: []
---



<p>Decision trees and Random Forests are some of the most widely used predictive models in data science. According to <a href="https://www.kaggle.com/surveys/2017">a survey taken by Kaggle in 2017</a> they only sit behind logistic regression as the 2nd and 3rd most used models at work. Due to their popularity, it is important to understand how tree based methods produce the results that drive many every-day decisions.</p>
<p>This post is largely inspired by the research of Hemant Ishwaran from the University of Miami. In particular, the following great work that explores the behavior of random forests and decision trees:</p>
<ul>
<li>H Ishwaran (2015) <a href="http://www.ccs.miami.edu/~hishwaran/papers/I.ML.2015.pdf">The effect of splitting on random forests</a>.</li>
</ul>
<div id="summary-of-post" class="section level3">
<h3>Summary of post</h3>
<ul>
<li>Introduction to decision trees</li>
<li>Splitting Criteria</li>
<li>Analysis of the effect this has in the regression setting</li>
<li>End-cut preference and its importance.</li>
</ul>
</div>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Tree based methods have one objective: partition the feature space into a series of rectangles where the target value is roughly constant. If we have a single feature <span class="math inline">\(X_1\)</span> these rectangles correspond to line segements of the form <span class="math inline">\(X_1 \in [a, b]\)</span>. For example, consider the following tree:</p>
<p>It groups the data into three line segements or regions: <span style="color: #ff7f0e"><span class="math inline">\(R_1 = [0, 0.2)\)</span></span>, <span style="color: #1f77b4"><span class="math inline">\(R_2 = [0.2, 0.75)\)</span></span>, and <span style="color: #2ca02c"><span class="math inline">\(R_3 = [0.75, 1]\)</span></span>. Data passes through the top of the tree (or root) and is split off until it falls into one of the three rectangles or leaves. Predictions are then made by returning a constant value in that leaf:</p>
<p><span class="math display">\[
\hat{f}(x) = \sum_{m=1}^3 c_m I\{x \in R_m\}
\]</span></p>
<p>Since the predictions of a decision tree is a simple function (the mean of the target in the leaves), what drives the accuracy of the model is its choice of splits.</p>
</div>
<div id="splitting-criteria" class="section level1">
<h1>Splitting Criteria</h1>
<p>The two most common splitting criteria are weighted variance reduction and the gini index.</p>
</div>
<div id="but-what-does-this-look-like" class="section level1">
<h1>But What Does this Look Like?</h1>
<pre class="r fold"><code>make_polynomial &lt;- function(n_samples = 100) {
  x &lt;- seq(-3, 3, length.out = n_samples)
  y &lt;- 2 * x^3 - 2 * x^2 - x
  dy &lt;- 6 * x^2 - 4 * x - 1
    
  list(x=x, y=y, dy=dy)
}

c(x, y, dy) %&lt;-% make_polynomial(n_samples = 500)

# fit a decision tree to the data
hyper_params = rpart::rpart.control(minbucket = 5, cp = 0.001)
tree &lt;- rpart::rpart(y ~ x, data = data.frame(x, y), 
                     control = hyper_params)
y_pred &lt;- predict(tree, data = data.frame(x, y))

# threshold values
splits = as.tibble(tree$splits)

# plot function and thresholds
ggplot(NULL, aes(x = x)) +
  geom_vline(aes(xintercept=splits$index), linetype = &#39;dashed&#39;, alpha = 0.5) + 
  geom_line(aes(y = dy), size = 1, color = &#39;darkgray&#39;) +
  geom_line(aes(y = y_pred), size = 1, color = &#39;mediumseagreen&#39;) +
  geom_line(aes(y = y), size = 1, color = &#39;darkorange&#39;) +
  ggthemes::theme_hc()</code></pre>
<p><img src="/post/2018-01-06-decision-tree-splits_files/figure-html/polynomial_data-1.png" width="672" /></p>
<p>Deciding the split</p>
<pre class="r"><code>biased_var &lt;- function(x) {
  n_samples &lt;- length(x)
  ((n_samples - 1) / n_samples) * var(x)
}

sum_of_squares &lt;- function(x) {
  (length(x) - 1) * var(x)
}

scan_split &lt;- function(X, y) {
  splits &lt;- sort(unique(X))
  loss &lt;- vector(&#39;numeric&#39;, length(splits))
  n_samples &lt;- length(X)
  for (split_idx in seq_along(splits)) {
    split &lt;- splits[split_idx]
    n_left &lt;- length(X &lt; split)
    n_right &lt;- length(X &gt;= split)
    loss [split_idx] &lt;- sum_of_squares(y) - (sum_of_squares(y[X &lt; split]) + sum_of_squares(y[X &gt;= split]))
  }
  
  list(loss, splits)
}

c(loss, splits) %&lt;-% scan_split(x, y)

ggplot(data.frame(loss, splits), aes(x = splits, y = loss)) +
  geom_line() +
  ggthemes::theme_hc()</code></pre>
<pre><code>## Warning: Removed 3 rows containing missing values (geom_path).</code></pre>
<p><img src="/post/2018-01-06-decision-tree-splits_files/figure-html/deciding_the_split-1.png" width="672" /></p>
<pre class="r"><code>data &lt;- data.frame(x=x, y=y, z = x &lt; -1.9)
g &lt;- ggplot(data, aes(x=z, y=y)) + 
  #geom_hline(aes(yintercept = mean(data$y[data$z]))) +
  #geom_hline(aes(yintercept = mean(data$y[!data$z]))) +
  geom_jitter() 
#ggMarginal(g)</code></pre>
<pre class="r fold"><code>rpart.plot::rpart.plot(tree)</code></pre>
<p><img src="/post/2018-01-06-decision-tree-splits_files/figure-html/decision_tree-1.png" width="672" /></p>
<pre class="python fold"><code>import matplotlib.pyplot as plt
import numpy as np
# !!space
from sklearn.utils import check_random_state
# !!space
def make_blocks(n_samples=400, noise_std=None, shuffle=True, random_state=123):
    &quot;&quot;&quot;Generates data from a piecewise blocking function. Common in wavelet analysis.
    
    Parameters
    ----------
    n_samples: int 
      Number of samples
      
    noise_std: float, optional (default=None)
      The standard deviation of the gausian noise applied to the data.
      
    shuffle: bool
      Whether to shuffle the data.
      
    random_state: int
      The seed to use for the random number generator.
    &quot;&quot;&quot;
    rng = check_random_state(random_state)
    
    X = np.linspace(0, 1, n_samples).reshape(-1, 1)
    # !!space
    def Kt(t):
        return (1 + np.sign(t)) / 2
    # !!space
    hj = np.array(
        [4, -5, 3, -4, 5, -4.2, 2.1, 4.3, -3.1, 2.1, -4.2])
    tj = np.array(
        [0.1, 0.13, 0.15, 0.23, 0.25, 0.40, 0.44, 0.65, 0.76, 0.78, 0.81])
    # !!space
    y = np.dot(Kt(X - tj), hj)
    if noise_std:
        y += noise_std * rng.randn(n_samples)
    # !!space
    return X, y
# !!space
X, y = make_blocks(noise_std=None)
# !!space
plt.plot(X, y)
plt.show()</code></pre>
<p><img src="/post/2018-01-06-decision-tree-splits_files/figure-html/block_data-1.png" width="672" /></p>
</div>
