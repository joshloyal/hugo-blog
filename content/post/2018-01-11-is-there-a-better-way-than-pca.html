---
title: Is There a Better Way Than PCA?
author: Joshua Loyal
date: '2018-01-11'
slug: is-there-a-better-way-than-pca
description: "Choosing the right unsupervised learning algorithm for dimension reduction is important. This post compares and contrasts sliced inverse regression (SIR) with principal component analysis (PCA) to demonstrate the effectiveness of sufficient dimension reduction."
categories: []
tags:
  - dimension reduction
  - sliced inverse regression
  - pca
  - machine learning
---



<p><strong>Note</strong>: Checkout the <a href="https://github.com/joshloyal/sliced"><code>sliced</code></a> python package I developed for this blog post!</p>
<p><em>Always use the right tool for the job.</em> This is useful guidance when building a machine learning model. Most data scientists would agree that support vector machines are able to classify images, but a convolutional neural network works better. Linear regression can fit a time-series, but an ARIMA model is more accurate. In each case what separates the approaches is that the second algorithm specifically address the structure of the problem. CNNs understand the translation invariance of an image and an ARIMA model takes advantage of the sequential nature of a time-series.</p>
<p>The logic is no different for dimension reduction. Performing dimension reduction for a supervised learning task should not be agnostic to the structure of the target. This insight motivates the paradigm of <strong>sufficient dimension reduction</strong>. In the remaining post I introduce the concept of sufficient dimension reduction through a toy example of a surface in three-dimensions. Then I compare a popular algorithm for SDR known as <strong>sliced inverse regression</strong> with principal component analysis (PCA) on a supervised learning problem.</p>
<div id="what-is-sufficient-dimension-reduction" class="section level1">
<h1>What is Sufficient Dimension Reduction?</h1>
<p>Let’s say we have a collection of features <span class="math inline">\(\mathbf{X}\)</span>, and we want to predict some target <span class="math inline">\(y\)</span>. In other words, we want to gain some insight about the conditional distribution <span class="math inline">\(y|\mathbf{X}\)</span>. When the number of features is high, it is common practice to remove irrelevant features before moving onto the prediction step. This removal of features is dimension reduction. We are <em>reducing</em> the number of features (aka <em>dimensions</em>) in our dataset.</p>
<p>Those familiar with dimension reduction may be thinking: “<em>I’ll use PCA or t-SNE for this!</em>”. While these techniques are great, they fall under the category of <em>unsupervised</em> dimension reduction. Notice that the unsupervised setup is different than the situation described above. Unsupervised learning finds structure in the distribution of <span class="math inline">\(\mathbf{X}\)</span> itself. No <span class="math inline">\(y\)</span> involved. For example, PCA reduces the number of features by identifying a small set of directions that explains the greatest variation in the data. This set of directions is known as a <strong>subspace</strong>. Of course, there is no reason to believe that this subspace contains any information about the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(\mathbf{X}\)</span>. Since PCA does not use information about <span class="math inline">\(y\)</span> when determining the directions of greatest variation, information about <span class="math inline">\(y\)</span> may be orthogonal to this space.</p>
<p>To avoid the situation above, <strong>sufficient dimension reduction</strong> keeps the relationship between <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(y\)</span> in mind. The goal is to find a small set of directions that can replace <span class="math inline">\(\mathbf{X}\)</span> without loss of information on the conditional distribution <span class="math inline">\(y|\mathbf{X}\)</span> <a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. This special subspace is called the <strong>dimension reducing subspace</strong> (DRS) or simply <span class="math inline">\(S(\mathbf{X})\)</span> <a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>. If we restrict our attention to the DRS we will find that the conditional distribution <span class="math inline">\(y|S(\mathbf{X})\)</span> is the same as the distribution <span class="math inline">\(y|\mathbf{X}\)</span>. But now we have a much smaller set of features! This allows us to better visualize our data and possibly gain deeper insights.</p>
<div id="example-a-quadratic-surface-in-3d" class="section level2">
<h2>Example: A Quadratic Surface in 3D</h2>
<p>Let’s develop some intuition about dimension reducing subspaces and how we can find them. Consider two features <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> that indicate the coordinates of a point on a two-dimensional plane. Associated with each pair of points, <span class="math inline">\((x_1, x_2)\)</span>, is the height of a surface that lies above it. Denote the height of the surface by the variable <span class="math inline">\(y\)</span>. The surface is a quadratic surface with <span class="math inline">\(y = x_1^2\)</span>. So the two features are (<span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>) and the target is the height of the quadratic, <span class="math inline">\(y\)</span>. The surface is displayed below:</p>
<pre class="python fold"><code>import numpy as np
# !!space
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
# !!space
# generate data y = X_1 ** 2 + 0 * X_2
def f(x, y):
    return x ** 2 
# !!space
x1 = np.linspace(0, 6, 30)
x2 = np.linspace(0, 6, 30)
# !!space
X1, X2 = np.meshgrid(x1, x2)
Y = f(X1, X2)
# !!space
# plot 3d surface y = x_1 ** 2
fig, ax = plt.subplots(figsize=(10,10), subplot_kw={&#39;projection&#39;:&#39;3d&#39;})
ax.plot_surface(X1, X2, Y, rstride=1, cstride=1,
                cmap=&#39;viridis&#39;, edgecolor=&#39;none&#39;)
# !!space
# rotate and label our axes
ax.view_init(35, -75)
ax.set_title(&#39;$y = x_1^2$&#39;);
ax.set_xlabel(&#39;$x_1$&#39;)
ax.set_ylabel(&#39;$x_2$&#39;)
ax.set_zlabel(&#39;y&#39;)
# !!space
plt.show()</code></pre>
<p><img src="/post/2018-01-11-is-there-a-better-way-than-pca_files/figure-html/3d_data_no_dir-1.png" width="960" /></p>
<p>To identify the dimension reducing subspace we start by answering the following question: what are all the possible subspaces associated with this dataset? They are the subspaces of the <span class="math inline">\(x_1x_2\)</span>-plane. This is a plane in two-dimensions. But the subspaces of a two-dimensional plane are all the one-dimensional lines that lie in that plane and that pass through the origin <span class="math inline">\((0, 0)\)</span>. The plot below displays a few examples:</p>
<pre class="python fold"><code>from loyalpy.plots import abline
from loyalpy.annotations import label_abline
# !!space
fig, ax = plt.subplots(figsize=(10, 10))
# !!space
# plot lines in the plane
ablines = [([-5, -5], [5, 5]),
           ([-5, 0], [5, 0]),
           ([0, -5], [0, 5]),
           ([-5, 5], [5, -5])]
for a_coords, b_coords in ablines:
    abline(a_coords, b_coords, ax=ax, ls=&#39;--&#39;, c=&#39;k&#39;, lw=2)
# !!space
# labels
label_abline([-5, 5], [5, -5], &#39;$\hat{\\beta}$ = (1, -1)&#39;, -2, 1.5)
label_abline([-5, -5], [5, 5], &#39;$\hat{\\beta}$ = (1, 1)&#39;, 1.5, 1.7)
label_abline([-5, 0], [5, 0], &#39; $x_1$-axis: $\hat{\\beta}$ = (1, 0)&#39;, -3, 0.2)
label_abline([0, 5], [0, -5], &#39;$x_2$-axis: $\hat{\\beta}$ = (0, 1)&#39;, 0.3, 2)
# !!space
ax.set_xlabel(&#39;$x_1$&#39;)
ax.set_ylabel(&#39;$x_2$&#39;)
ax.set_xlim(-5, 5)
ax.set_ylim(-5, 5)
# !!space
plt.show()</code></pre>
<p><img src="/post/2018-01-11-is-there-a-better-way-than-pca_files/figure-html/subspace_demo-1.png" width="960" /></p>
<p>Of all these subspaces there is a single dimension reducing subspace. But what is it? Recall that <span class="math inline">\(y = x_1^2\)</span>. This is a function of only <span class="math inline">\(x_1\)</span>. Since <span class="math inline">\(y\)</span> does not depend on <span class="math inline">\(x_2\)</span>, we could drop <span class="math inline">\(x_2\)</span> from further analysis and still be able to compute <span class="math inline">\(y\)</span>. This means that <span class="math inline">\(x_1\)</span> contains all the information about <span class="math inline">\(y\)</span>. In the language of subspaces: dropping <span class="math inline">\(x_2\)</span> corresponds to projecting the data onto the <span class="math inline">\(x_1\)</span>-axis. But the <span class="math inline">\(x_1\)</span>-axis is a valid subspace. Therefore, the dimension reducing subspace is the <span class="math inline">\(x_1\)</span>-axis!</p>
<p>But how would a machine learning algorithm tell us to project the data onto the <span class="math inline">\(x_1\)</span>-axis? The outputs of the algorithm are numbers not subspaces or projections. In the plot above you may have noticed that each subspace is labeled with a vector <span class="math inline">\(\hat{\beta}\)</span>. This vector tells us the direction of the line associated with the subspace. In other words, the vector <strong>spans</strong> that subspace. More importantly, this vector can project data onto its associated subspace. Just take the dot product of <span class="math inline">\(\hat{\beta}\)</span> with the data. For example to project a point onto the <span class="math inline">\(x_1\)</span>-axis let <span class="math inline">\(\hat{\beta} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}\)</span>, so that <span class="math display">\[
\hat{\beta}^T x = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = x_1.
\]</span></p>
<p>Therefore, finding the dimension reducing subspace boils down to identifying the direction vector <span class="math inline">\(\hat{\beta}\)</span>. If we can estimate <span class="math inline">\(\hat{\beta}\)</span>, then we can create a lower dimensional dataset by using <span class="math inline">\(\mathbf{X}\hat{\beta}\)</span> in our analysis instead of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>To see how we can visually identify the dimension reducing subspace lets look at the surface <span class="math inline">\(y = x_1^2\)</span> again. This time I’ve labeled the direction of the dimension reducing subspace with an arrow.</p>
<pre class="python fold"><code>import numpy as np
# !!space
from mpl_toolkits.mplot3d import Axes3D
from loyalpy.annotations import Arrow3D
# !!space
# generate data y = X_1 ** 2 + 0 * X_2
def f(x, y):
    return x ** 2 
# !!space
x1 = np.linspace(0, 6, 30)
x2 = np.linspace(0, 6, 30)
# !!space
X1, X2 = np.meshgrid(x1, x2)
Y = f(X1, X2)
# !!space
# plot 3d surface y = x_1 ** 2
fig, ax = plt.subplots(figsize=(10,10), subplot_kw={&#39;projection&#39;:&#39;3d&#39;})
ax.plot_surface(X1, X2, Y, rstride=1, cstride=1,
                cmap=&#39;viridis&#39;, edgecolor=&#39;none&#39;)
# !!space
# An arrow indicating the central subspace
arrow = Arrow3D([0, 6], [3, 3],
                [0, 0], mutation_scale=20, 
                lw=3, arrowstyle=&quot;-|&gt;&quot;, color=&quot;k&quot;)
ax.add_artist(arrow)
ax.text(4, 3.5, 0, &quot;$\hat{\\beta} = (1, 0)$&quot;, (1, 0, 0), 
        color=&#39;k&#39;, fontsize=12)
# !!space
# rotate and label our axes
ax.view_init(35, -75)
ax.set_title(&#39;$y = x_1^2$&#39;);
ax.set_xlabel(&#39;$x_1$&#39;)
ax.set_ylabel(&#39;$x_2$&#39;)
ax.set_zlabel(&#39;y&#39;)
# !!space
plt.show()</code></pre>
<p><img src="/post/2018-01-11-is-there-a-better-way-than-pca_files/figure-html/3d_data-1.png" width="960" /></p>
<p>Notice how the height of the surface does not change along the <span class="math inline">\(x_2\)</span> dimension. This is why <span class="math inline">\(x_2\)</span> carries no information about <span class="math inline">\(y\)</span>. Now look at the direction associated with the dimension reducing subspace, <span class="math inline">\(\hat{\beta} = (1, 0)\)</span> or the <span class="math inline">\(x_1\)</span>-axis. Notice how all the variation in <span class="math inline">\(y\)</span> occurs along this line. This is why <span class="math inline">\(\hat{\beta}\)</span> carries all the information about <span class="math inline">\(y\)</span>.</p>
<p>Now look at what happens when we rotate the plot so that the <span class="math inline">\(x\)</span>-axis is aligned with the direction of <span class="math inline">\(\hat{\beta}\)</span>. This rotation is equivalent to projecting the data onto the dimension reducing subspace. This rotation / projection is shown below:</p>
<pre class="python fold"><code># plot the surface
ax = plt.axes(projection=&#39;3d&#39;)
ax.plot_surface(X1, X2, Y, rstride=1, cstride=1,
                cmap=&#39;viridis&#39;, edgecolor=&#39;none&#39;)
# !!space
# label beta
arrow = Arrow3D([0, 6], [3, 3],
                [0, 0], mutation_scale=20, 
                lw=3, arrowstyle=&quot;-|&gt;&quot;, color=&quot;k&quot;)
ax.add_artist(arrow)
ax.text(4, 3.5, 2, &quot;$\hat{\\beta} = (1, 0)$&quot;, (1, 0, 0), color=&#39;k&#39;, fontsize=12)
# !!space
# label the axes and rotate our view
ax.view_init(0, -90)
ax.set_title(&#39;$y = x_1^2$&#39;);
ax.set_xlabel(&#39;$x_1$&#39;)
ax.set_ylabel(&#39;$x_2$&#39;)
ax.set_zlabel(&#39;y&#39;)
# !!space
plt.show()</code></pre>
<p><img src="/post/2018-01-11-is-there-a-better-way-than-pca_files/figure-html/central_subspace-1.png" width="960" /></p>
<p>Something amazing happened! Although the plot is a two-dimensional surface, it appears like a one-dimensional curve! Each value on the <span class="math inline">\(\hat{\beta}\)</span>-axis corresponds to only a single <span class="math inline">\(y\)</span> value. If we chose any other direction the function would not be one-to-one. Furthermore, the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x_1\)</span> is clear in this space. They have a quadratic relationship. In other words, this projection reduces the number of features from two to one without compromising the functional form <span class="math inline">\(y = x_1^2\)</span>. This is what it looks like when the reduction of features is sufficient!</p>
</div>
</div>
<div id="sliced-inverse-regression-vs.-pca" class="section level1">
<h1>Sliced Inverse Regression VS. PCA</h1>
<p>Now that we’ve seen what sufficient dimension reduction can accomplish, we are ready to explore the differences between <em>unsupervised</em> and <em>sufficient</em> dimension reduction. To do this I chose to compare PCA with an algorithm known as <strong>sliced inverse regression</strong> (SIR). SIR was developed in 1991 by Ker-Chau Li from UCLA <a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> and expanded upon by Dennis Cook and Sanford Weisberg from the University of Minnesota <a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>. It uses the idea of inverse regression to estimate the dimension reducing subspace of the data. I use the <code>SlicedInverseRegression</code> code in the <a href="https://github.com/joshloyal/sliced"><code>sliced</code></a> package to fit a SIR model. PCA is carried out using <code>sklearn</code>’s <code>PCA</code> implementation.</p>
<div id="the-data" class="section level2">
<h2>The Data</h2>
<p>Consider the following data generating process:</p>
<p><span class="math display">\[
y = \sin(0.7 X_1 - 0.7 X_2) + \epsilon
\]</span> <span class="math display">\[
X_i \overset{iid}\sim N(0, 1), \quad \epsilon \overset{iid}\sim N(0, 0.1) 
\]</span></p>
<p>The dataset consists of two uncorrelated features, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, sampled from a normal distribution. The target is a sine function applied to a linear combination of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. There is also independent gaussian noise, <span class="math inline">\(\epsilon\)</span>, applied to the sinusoidal signal.</p>
<p>A scatterplot of <span class="math inline">\(X_2\)</span> vs. <span class="math inline">\(X_1\)</span> is displayed below. The points are colored according to <span class="math inline">\(y\)</span>. Brighter colors correspond to larger values of the target. The dimension reducing subspace is labeled with a dashed line.</p>
<pre class="python fold"><code>import numpy as np
import matplotlib.pyplot as plt
# !!space
from loyalpy.annotations import label_line, label_abline
# !!space
np.random.seed(123)
# !!space
n_samples = 500
X = np.random.randn(n_samples, 2)
y = np.sin(0.7 * X[:, 0] - 0.7 * X[:, 1]) + 0.1 * np.random.randn(n_samples)
# !!space
fig, ax = plt.subplots(figsize=(10, 10))
# !!space
# label the central subspace
line, = ax.plot([-5, 5], [5, -5], ls=&#39;--&#39;, c=&#39;k&#39;, alpha=0.5)
label_line(line, &#39;dimension reducing subspace&#39;, -4, 2)
# !!space
# scatter plot of points
ax.scatter(X[:, 0], X[:, 1], c=y, cmap=&#39;viridis&#39;, edgecolors=&#39;k&#39;, s=80)
ax.set_xlim(-5, 5)
ax.set_ylim(-5, 5)
ax.set_xlabel(&#39;$X_1$&#39;)
ax.set_ylabel(&#39;$X_2$&#39;)
# !!space
plt.show()</code></pre>
<p><img src="/post/2018-01-11-is-there-a-better-way-than-pca_files/figure-html/data-1.png" width="960" /></p>
<p>Notice how the dimension reducing subspace follows the color gradient of the target. This behavior distinguishes the dimension reducing subspace. It maximally preserves the information in <span class="math inline">\(y\)</span>.</p>
</div>
<div id="what-should-we-see" class="section level2">
<h2>What Should We See?</h2>
<p>Let’s take a step back and identify the dimension reducing subspace assuming the data generating process is known. According to the equations above the mean of <span class="math inline">\(y\)</span> is determined by a single feature:</p>
<p><span class="math display">\[
Z = 0.7X_1 - 0.7X_2.
\]</span></p>
<p>For example, if <span class="math inline">\(Z = \pi\)</span>, then <span class="math inline">\(E[y|Z] = \sin(Z) = \sin(\pi) = 0\)</span>. This demonstrates that the conditional expectation is a function of a single variable, <span class="math inline">\(Z\)</span>, instead of the two variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. This variable reduce the dimension of the dataset from two to one without losing any information about <span class="math inline">\(y\)</span>.</p>
<p>But <span class="math inline">\(Z\)</span> is a derived feature not the dimension reducing subspace or its associated direction. So what direction is associated with the variable <span class="math inline">\(Z\)</span>? Notice that <span class="math inline">\(Z\)</span> is associated with the vector <span class="math inline">\(\hat{\beta} = (0.7, -0.7)\)</span>. This is because <span class="math inline">\(Z\)</span> is calculated by carrying out the product:</p>
<p><span class="math display">\[
Z = \hat{\beta}^T X = \begin{pmatrix} 0.7 &amp; -0.7 \end{pmatrix} \begin{pmatrix} X_1 \\ X_2 \end{pmatrix} = 0.7 X_1 - 0.7 X_2.
\]</span></p>
<p>Therefore, the dimension reducing subspace is the one dimensional subspace spanned by the vector <span class="math inline">\(\hat{\beta} = (1, -1)\)</span>.</p>
</div>
<div id="estimating-hatbeta" class="section level2">
<h2>Estimating <span class="math inline">\(\hat{\beta}\)</span></h2>
<p>Of course we do not know the data generating process or that <span class="math inline">\(\hat{\beta}\)</span> exists when setting out to perform an analysis. Nevertheless, we still want to reduce the number of features in this dataset from two to one. Both SIR and PCA will accomplish our goal. They will project our data into a one dimensional subspace. But which one is better? From the analysis above we know the best subspace is <span class="math inline">\(\hat{\beta} = (1, -1)\)</span> , which is also the dimension reducing subspace that SIR estimates. But how off will PCA be from this subspace? If PCA is close, then who cares about all this extra theory? Let’s fit both algorithms and compare the results to find out.</p>
<p>Fitting the algorithms is simple since they both adhere to the <code>sklearn</code> transformer API. A single hyperparameter <code>n_components</code> indicates the dimension of the subspace. In this case we want a single direction, so we set <code>n_components=1</code>. The <code>components_</code> attribute stores the estimated direction of the subspace once the algorithm is fit. The only difference between the algorithms is that SIR needs to know about the target during the fitting process. The following code fits the two algorithms and extracts the estimated <span class="math inline">\(\hat{\beta}\)</span> vector:</p>
<pre class="python"><code>from sliced import SlicedInverseRegression
from sklearn.decomposition import PCA
# !!space
# fit sliced&#39;s SIR. Remember we need to pass the target in as well!
sir = SlicedInverseRegression(n_components=1).fit(X, y)
sir_direction = sir.components_[0, :]
# !!space
# fit sklearn&#39;s PCA. Unsupervised so it has no knowledge of y.
pca = PCA(n_components=1).fit(X)
pca_direction = pca.components_[0, :]</code></pre>
<p>With the models fit to the data, we can compare the directions found by PCA and SIR:</p>
<pre class="python fold"><code># scatter plot of points
fig, ax = plt.subplots(figsize=(10,10))
ax.scatter(X[:, 0], X[:, 1], c=y, cmap=&#39;viridis&#39;, alpha=0.2, edgecolors=&#39;k&#39;, s=80)
ax.set_xlim(-5, 5)
ax.set_ylim(-5, 5)
ax.set_xlabel(&#39;$X_1$&#39;)
ax.set_ylabel(&#39;$X_2$&#39;)
# !!space
# label the central subspace
line, = ax.plot([-5, 5], [5, -5], ls=&#39;--&#39;, c=&#39;k&#39;, alpha=0.5)
label_line(line, &#39;dimension reducing subspace&#39;, -4, 2)
# !!space
# label subspaces found by PCA and SIR
arrow_aes = dict(head_width=0.2,
                 head_length=0.2,
                 width=0.08,
                 ec=&#39;k&#39;)
ax.arrow(0, 0, pca_direction[0], pca_direction[1], fc=&#39;darkorange&#39;, **arrow_aes)
label_abline((0, 0), pca_direction, &#39;PCA&#39;, -0.7, -0.2, color=&#39;darkorange&#39;, outline=True)
# !!space
ax.arrow(0, 0, sir_direction[0], sir_direction[1], fc=&#39;deepskyblue&#39;, **arrow_aes)
label_abline((0, 0), sir_direction, &#39;SIR&#39;, 0.5, -0.5, color=&#39;deepskyblue&#39;, outline=True)
# !!space
plt.show()</code></pre>
<p><img src="/post/2018-01-11-is-there-a-better-way-than-pca_files/figure-html/directions-1.png" width="960" /></p>
<p>The <span style="color: darkorange">orange arrow</span> corresponds to the subspace found by PCA, while the <span style="color: deepskyblue">blue arrow</span> corresponds to the subspace found by SIR. Notice how the direction found by PCA has nothing to do with <span class="math inline">\(y\)</span>. If it did, then it would point along the color gradient. Instead PCA picks the direction that has the largest spread along the data cloud. In contrast, SIR knows about <span class="math inline">\(y\)</span>. This information allows SIR to orients itself nicely along the color gradient, which contains all the information about the target.</p>
<p>In fact, the performance of PCA is even worse than it appears. Since the data is generated from an isotropic gaussian blob, the PCA directions will be completely random upon replication. PCA chooses the direction that happens to have the most variance in the sample. But the data is isotropic, so this direction is uniformly distributed over all possible subspaces. The variance of the first principal component is huge! This is catastrophic if you are an analyst trying to make sense of the data. You may trick yourself into thinking the direction chosen by PCA has some special meaning, when in fact it does not. To see this more clearly I repeated this experiment multiple times. Each time I measured the angle between the dimension reducing subspace, <span class="math inline">\(\hat{\beta}_{true}\)</span>, and the direction chosen by PCA and SIR. I then displayed the distribution of the angle in a histogram. The result is displayed below:</p>
<pre class="python fold"><code>def normalize_it(vec):
  &quot;&quot;&quot;Normalize a vector.&quot;&quot;&quot;
  vec = vec.astype(np.float64)
  vec /= np.linalg.norm(vec)
  return vec
# !!space
def replicate_angle(n_samples=500, n_iter=2500):
    &quot;&quot;&quot;Caculate the SIR and PCA angles for `n_iter` replications.&quot;&quot;&quot;
    # direction of dimension reducing subspace
    true_direction = normalize_it(np.array([1, -1]))
    # !!space
    # iterate the experiment multiple times and see how the angle changes
    pca_angle = np.empty(n_iter, dtype=np.float64)
    sir_angle = np.empty(n_iter, dtype=np.float64)
    for i in range(n_iter):
        np.random.seed(i)
        # !!space
        X = np.random.randn(n_samples, 2)
        y = np.sin(0.7 * X[:, 0] - 0.7 * X[:, 1]) + 0.1 * np.random.randn(n_samples)
        # !!space
        sir = SlicedInverseRegression(n_components=1).fit(X, y)
        sir_direction = normalize_it(sir.components_[0, :])
        # !!space
        pca = PCA(n_components=1).fit(X)
        pca_direction = normalize_it(pca.components_[0, :])
        # !!space
        cos_angle = np.dot(pca_direction, true_direction)
        pca_angle[i] = np.arccos(cos_angle)
        # !!space
        cos_angle = np.dot(sir_direction, true_direction)
        sir_angle[i] = np.arccos(cos_angle)
    
    return pca_angle, sir_angle
# !!space
# plot the results
pca_angle, sir_angle = replicate_angle()
# !!space
fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10, 10))
# !!space
# less bins since everything is basically at zero within precision
ax1.hist(sir_angle, color=&quot;steelblue&quot;, edgecolor=&quot;white&quot;, linewidth=2, bins=3)
ax1.set_title(&#39;SIR&#39;)
ax1.set_xlabel(&#39;Angle between $\hat{\\beta}_{true}$ and $\hat{\\beta}_{sir}$&#39;)
ax1.set_ylabel(&#39;Counts&#39;)
# !!space
ax2.hist(pca_angle, color=&quot;steelblue&quot;, edgecolor=&quot;white&quot;, linewidth=2, bins=20)
ax2.set_title(&#39;PCA&#39;)
ax2.set_xlabel(&#39;Angle between $\hat{\\beta}_{true}$ and $\hat{\\beta}_{pca}$&#39;)
ax2.set_ylabel(&#39;Counts&#39;)
# !!space
plt.show()</code></pre>
<p><img src="/post/2018-01-11-is-there-a-better-way-than-pca_files/figure-html/replicate_angle-1.png" width="960" /></p>
<p>As expect the directions estimated by PCA (the histogram on the bottom) are uniformly distributed from 0 to <span class="math inline">\(2\pi\)</span>. The direction found by PCA is meaningless. In contrast, the distribution of the angle found by SIR (the histogram on the top) concentrates itself around zero. Meaning it almost always finds the direction of the dimension reducing subspace for this problem.</p>
<p>Finally, let’s take a look at what the data looks like in the dimension reducing subspace found by SIR and the subspace found by PCA. The <code>transform</code> method projects the data into these subspaces:</p>
<pre class="python"><code># project data into the subspaces identified by SIR and PCA
X_sir = sir.transform(X)
X_pca = pca.transform(X)</code></pre>
<p>The relationship between these new features and <span class="math inline">\(y\)</span> is visualized in a two-dimensional scatterplot:</p>
<pre class="python fold"><code>f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(20, 10))
# !!space
ax1.scatter(X_sir[:, 0], y, c=y, cmap=&#39;viridis&#39;, edgecolors=&#39;k&#39;, s=80)
ax1.set_title(&#39;SIR Subspace&#39;)
ax1.set_xlabel(&#39;$\mathbf{X}\hat{\\beta}_{sir}$&#39;)
ax1.set_ylabel(&#39;y&#39;)
# !!space
ax2.scatter(X_pca[:, 0], y, c=y, cmap=&#39;viridis&#39;, edgecolors=&#39;k&#39;, s=80)
ax2.set_title(&#39;PCA Subspace&#39;)
ax2.set_xlabel(&#39;$\mathbf{X}\hat{\\beta}_{pca}$&#39;)
ax2.set_ylabel(&#39;y&#39;)
# !!space
plt.show()</code></pre>
<p><img src="/post/2018-01-11-is-there-a-better-way-than-pca_files/figure-html/projected_plots-1.png" width="1920" /></p>
<p>The power of the SIR algorithm is evident in these plots.The sinusoidal pattern that links the features with the target is visible in the SIR plot on the left. This pattern is almost washed out in the PCA plot on the right. In this plot the data is spread almost uniformly about the plane. PCA destroys the relationship between the features and the target, while SIR preserves it.</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>It is important to think critically about the dimension reduction algorithm used to analyze a dataset. The algorithm should reduce the features in a way that preserves the information of interest instead of finding spurious structure. <strong>Sufficient dimension reduction</strong> and <strong>sliced inverse regression</strong> are one such way of looking for structure in a supervised learning setting.</p>
<p>However, you should always remain diligent. These algorithms are not perfect. In particular, SIR fails to estimate the dimension reducing subspace when the conditional density <span class="math inline">\(\mathbf{X}|y\)</span> is symmetric in <span class="math inline">\(y\)</span>. The SAVE algorithm (also in the <a href="https://github.com/joshloyal/sliced"><code>sliced</code></a> package) overcomes this limitation. However, SAVE fails to pick up on simple linear trends easily estimated by SIR. If possible both SIR and SAVE should be used together in practice. Of course, I did not actually go through the mechanics of the SIR algorithm. That is the subject of a future blog post.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This may sound similar to a <em>sufficient statistic</em> in mathematical statistics. In fact it should! They are both based on the same idea of sufficiency.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Note that there can be more than one dimension reducing subspace. Typically we are interested in the smallest dimension reducing subspace, which may not even exist. If it does then it is unique and called the <strong>central subspace</strong>. This is analogous to a minimum sufficient statistic in mathematical statistics.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Li, K C. (1991) “Sliced Inverse Regression for Dimension Reduction (with discussion)”, Journal of the American Statistical Association, 86, 316-342.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Cook, D. and Weisberg , S. (1991) “Sliced Inverse Regression for Dimension Reduction: Comment”, Journal of the American Statistical Association, 86, 328-342.<a href="#fnref4">↩</a></p></li>
</ol>
</div>
